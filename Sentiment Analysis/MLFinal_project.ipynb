{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports and nltk download"
      ],
      "metadata": {
        "id": "zENTBF23Lqbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EOtbcDaF74c",
        "outputId": "1c3b8362-c47a-47aa-d798-43326af88ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import nltk as nltk\n",
        "nltk.download('all')\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the google drive"
      ],
      "metadata": {
        "id": "eJUYa8PIMEhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HUXA2WMMD99",
        "outputId": "38e1dad2-fde0-44d6-b313-08352e20487b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File read"
      ],
      "metadata": {
        "id": "7SS4lAAALk8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/My Drive/Final project/sentiment+labelled+sentences/sentiment labelled sentences/amazon_cells_labelled.txt\"  # Update this with your file path\n",
        "dfAmazon = pd.read_csv(file_path, sep='\\t', header=None, names=['text', 'sentiment'])\n",
        "file_path = \"/content/drive/My Drive/Final project/sentiment+labelled+sentences/sentiment labelled sentences/imdb_labelled.txt\"\n",
        "dfIMDB = pd.read_csv(file_path, sep='\\t', header=None, names=['text', 'sentiment'])\n",
        "file_path = \"/content/drive/My Drive/Final project/sentiment+labelled+sentences/sentiment labelled sentences/yelp_labelled.txt\"\n",
        "dfYelp = pd.read_csv(file_path, sep='\\t', header=None, names=['text', 'sentiment'])\n",
        "df_combined = pd.concat([dfAmazon, dfIMDB, dfYelp], ignore_index=True)"
      ],
      "metadata": {
        "id": "QwqXdezqLfY1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AROLuDSXLjwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "6o517KQ6LkLF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalnum()]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "Qze3j0sOQz5-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfAmazon['text'] = dfAmazon['text'].apply(preprocess_text)\n",
        "dfIMDB['text'] = dfIMDB['text'].apply(preprocess_text)\n",
        "dfYelp['text'] = dfYelp['text'].apply(preprocess_text)\n",
        "df_combined['text'] = df_combined['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "AVaeV7VdQ3aE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_trainAmazon, X_testAmazon, y_trainAmazon, y_testAmazon = train_test_split(dfAmazon['text'], dfAmazon['sentiment'], test_size=0.2, random_state=42)\n",
        "X_trainIMDB, X_testIMDB, y_trainIMDB, y_testIMDB = train_test_split(dfIMDB['text'], dfIMDB['sentiment'], test_size=0.2, random_state=42)\n",
        "X_trainYelp, X_testYelp, y_trainYelp, y_testYelp = train_test_split(dfYelp['text'], dfYelp['sentiment'], test_size=0.2, random_state=42)\n",
        "X_trainAll, X_testAll, y_trainAll, y_testAll = train_test_split(df_combined['text'], df_combined['sentiment'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tcM5rgRiRGGW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidfAmazon = tfidf_vectorizer.fit_transform(X_trainAmazon)\n",
        "X_test_tfidfAmazon = tfidf_vectorizer.transform(X_testAmazon)\n",
        "\n",
        "X_train_tfidfIMDB = tfidf_vectorizer.fit_transform(X_trainIMDB)\n",
        "X_test_tfidfIMDB = tfidf_vectorizer.transform(X_testIMDB)\n",
        "\n",
        "X_train_tfidfYelp = tfidf_vectorizer.fit_transform(X_trainYelp)\n",
        "X_test_tfidfYelp = tfidf_vectorizer.transform(X_testYelp)\n",
        "\n",
        "X_train_tfidfAll = tfidf_vectorizer.fit_transform(X_trainAll)\n",
        "X_test_tfidfAll = tfidf_vectorizer.transform(X_testAll)"
      ],
      "metadata": {
        "id": "dbybzLCZSc4m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_xgb = XGBClassifier()\n",
        "clf_xgb.fit(X_train_tfidfAmazon, y_trainAmazon).fit(X_train_tfidfAmazon, y_trainAmazon)\n",
        "y_predAmazon = clf_xgb.predict(X_test_tfidfAmazon)\n",
        "clf_xgb.fit(X_train_tfidfIMDB, y_trainIMDB)\n",
        "y_predIMDB = clf_xgb.predict(X_test_tfidfIMDB)\n",
        "clf_xgb.fit(X_train_tfidfYelp, y_trainYelp)\n",
        "y_predYelp = clf_xgb.predict(X_test_tfidfYelp)\n",
        "clf_xgb.fit(X_train_tfidfAll, y_trainAll)\n",
        "y_predAll = clf_xgb.predict(X_test_tfidfAll)"
      ],
      "metadata": {
        "id": "di6IJrKvYm3u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracyAmazon = accuracy_score(y_testAmazon, y_predAmazon)\n",
        "print(\"Amazon Accuracy:\", accuracyAmazon)\n",
        "accuracyIMDB = accuracy_score(y_testIMDB, y_predIMDB)\n",
        "print(\"IMDB Accuracy:\", accuracyIMDB)\n",
        "accuracyYelp = accuracy_score(y_testYelp, y_predYelp)\n",
        "print(\"Yelp Accuracy:\", accuracyYelp)\n",
        "accuracyAll = accuracy_score(y_testAll, y_predAll)\n",
        "print(\"All:\", accuracyAll)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5TzvDesZLAG",
        "outputId": "9780efd7-b6a9-4e9a-eecd-1b342521b263"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon Accuracy: 0.7\n",
            "IMDB Accuracy: 0.62\n",
            "Yelp Accuracy: 0.675\n",
            "All: 0.7709090909090909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_tfidfAmazon, y_trainAmazon).fit(X_train_tfidfAmazon, y_trainAmazon)\n",
        "y_predAmazon = clf.predict(X_test_tfidfAmazon)\n",
        "clf.fit(X_train_tfidfIMDB, y_trainIMDB)\n",
        "y_predIMDB = clf.predict(X_test_tfidfIMDB)\n",
        "clf.fit(X_train_tfidfYelp, y_trainYelp)\n",
        "y_predYelp = clf.predict(X_test_tfidfYelp)\n",
        "clf.fit(X_train_tfidfAll, y_trainAll)\n",
        "y_predAll = clf.predict(X_test_tfidfAll)"
      ],
      "metadata": {
        "id": "3ask88c8H5qI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracyAmazon = accuracy_score(y_testAmazon, y_predAmazon)\n",
        "print(\"Amazon Accuracy:\", accuracyAmazon)\n",
        "accuracyIMDB = accuracy_score(y_testIMDB, y_predIMDB)\n",
        "print(\"IMDB Accuracy:\", accuracyIMDB)\n",
        "accuracyYelp = accuracy_score(y_testYelp, y_predYelp)\n",
        "print(\"Yelp Accuracy:\", accuracyYelp)\n",
        "accuracyAll = accuracy_score(y_testAll, y_predAll)\n",
        "print(\"All:\", accuracyAll)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az996YnKIE6Z",
        "outputId": "75fc1900-ea97-42be-fef2-a0feeb1abcdb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon Accuracy: 0.79\n",
            "IMDB Accuracy: 0.7466666666666667\n",
            "Yelp Accuracy: 0.765\n",
            "All: 0.7654545454545455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_rf = RandomForestClassifier()\n",
        "clf_rf.fit(X_train_tfidfAmazon, y_trainAmazon).fit(X_train_tfidfAmazon, y_trainAmazon)\n",
        "y_predAmazon = clf_rf.predict(X_test_tfidfAmazon)\n",
        "clf_rf.fit(X_train_tfidfIMDB, y_trainIMDB)\n",
        "y_predIMDB = clf_rf.predict(X_test_tfidfIMDB)\n",
        "clf_rf.fit(X_train_tfidfYelp, y_trainYelp)\n",
        "y_predYelp = clf_rf.predict(X_test_tfidfYelp)\n",
        "clf_rf.fit(X_train_tfidfAll, y_trainAll)\n",
        "y_predAll = clf_rf.predict(X_test_tfidfAll)"
      ],
      "metadata": {
        "id": "sATI7OZtIKao"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracyAmazon = accuracy_score(y_testAmazon, y_predAmazon)\n",
        "print(\"Amazon Accuracy:\", accuracyAmazon)\n",
        "accuracyIMDB = accuracy_score(y_testIMDB, y_predIMDB)\n",
        "print(\"IMDB Accuracy:\", accuracyIMDB)\n",
        "accuracyYelp = accuracy_score(y_testYelp, y_predYelp)\n",
        "print(\"Yelp Accuracy:\", accuracyYelp)\n",
        "accuracyAll = accuracy_score(y_testAll, y_predAll)\n",
        "print(\"All:\", accuracyAll)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgzTL1pYIfVh",
        "outputId": "8fa53379-ed94-4d3e-a29f-f48e21ec66e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon Accuracy: 0.74\n",
            "IMDB Accuracy: 0.7133333333333334\n",
            "Yelp Accuracy: 0.755\n",
            "All: 0.7672727272727272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_log = LogisticRegression()\n",
        "clf_log.fit(X_train_tfidfAmazon, y_trainAmazon).fit(X_train_tfidfAmazon, y_trainAmazon)\n",
        "y_predAmazon = clf_log.predict(X_test_tfidfAmazon)\n",
        "clf_log.fit(X_train_tfidfIMDB, y_trainIMDB)\n",
        "y_predIMDB = clf_log.predict(X_test_tfidfIMDB)\n",
        "clf_log.fit(X_train_tfidfYelp, y_trainYelp)\n",
        "y_predYelp = clf_log.predict(X_test_tfidfYelp)\n",
        "clf_log.fit(X_train_tfidfAll, y_trainAll)\n",
        "y_predAll = clf_log.predict(X_test_tfidfAll)"
      ],
      "metadata": {
        "id": "M7LU66M5Ivv4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracyAmazon = accuracy_score(y_testAmazon, y_predAmazon)\n",
        "print(\"Amazon Accuracy:\", accuracyAmazon)\n",
        "accuracyIMDB = accuracy_score(y_testIMDB, y_predIMDB)\n",
        "print(\"IMDB Accuracy:\", accuracyIMDB)\n",
        "accuracyYelp = accuracy_score(y_testYelp, y_predYelp)\n",
        "print(\"Yelp Accuracy:\", accuracyYelp)\n",
        "accuracyAll = accuracy_score(y_testAll, y_predAll)\n",
        "print(\"All:\", accuracyAll)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZkstbCzI71P",
        "outputId": "26b3b9e0-9e77-4c9c-8154-5ad4c658d67b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon Accuracy: 0.77\n",
            "IMDB Accuracy: 0.76\n",
            "Yelp Accuracy: 0.755\n",
            "All: 0.7727272727272727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_NB_WORDS = 10000\n",
        "MAX_SEQUENCE_LENGTH = 100"
      ],
      "metadata": {
        "id": "fE6q6rx1lkfh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(X_trainAll)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_trainAll)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_testAll)"
      ],
      "metadata": {
        "id": "tMXDcEGxlmg6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "2e9NujHyloTb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_pad, y_trainAll, epochs=5, batch_size=64, validation_data=(X_test_pad, y_testAll))\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_testAll)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rtFogbQlp6h",
        "outputId": "cbc7b30d-cd23-4554-f79e-4853ac9d2365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "17/35 [=============>................] - ETA: 4s - loss: 0.6929 - accuracy: 0.5129"
          ]
        }
      ]
    }
  ]
}